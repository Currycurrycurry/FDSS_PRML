# Lab1第一部分：反向传播算法 说明文档

17302010063 黄佳妮 软件工程

## 简介

该lab第一部分要求我们在实现bp算法的基础之上，手动设计一个可调节网络层数和设置参数的全连接网络（FNN/MLP）。

本文档分为如下五部分：

- 学习笔记
- 反向传播算法推导
- 代码架构说明
- 实验优化说明
- 总结反思

## 学习笔记

### 激活函数

作为一种非线性映射，是原始感知机（Perceptron）的一种进化，使原来简单的矩阵相乘叠加的线性组合（只能用于拟合线性函数），进化为可以逼近任意非线性函数的神经网络。

#### 非线性激活函数

使用非线性激活函数是为了增加神经网络模型的非线性因素，hidden layer使用非线性激活函数。

#### sigmoid

![[公式]](https://www.zhihu.com/equation?tex=%5Csigma%28x%29%3D%5Cfrac%7B1%7D%7B1%2Be%5E%7B-x%7D%7D)

又称 Logistic函数，用于隐层神经元输出，取值范围为(0,1)，可以用来做二分类。

- 导数

![img](https://pic1.zhimg.com/80/v2-e2fbdc1a61f770bc462d0622b65fe66c_1440w.jpg)

- 图像

  ![img](https://pic2.zhimg.com/80/v2-6d06c0db16a0cc66e27bfb97d08b14bd_1440w.jpg)

- 优点

  1. Sigmoid函数的输出在(0,1)之间，输出范围有限，优化稳定，可以用作输出层。
  2. 连续函数，便于求导。

- 缺点

  1. sigmoid函数在变量取绝对值非常大的正值或负值时会出现**饱和**现象，意味着函数会变得很平，并且对输入的微小改变会变得不敏感。

  在**反向传播**时，当梯度接近于0，权重基本不会更新，很容易就会出现**梯度消失**的情况，从而无法完成深层网络的训练。

  - 梯度消失问题：当梯度小于1时，预测值与真实值之间的误差每传播一层会衰减一次，如果在深层模型中使用sigmoid作为激活函数，这种现象尤为明显，将导致模型收敛停滞不前。

  2. **sigmoid函数的输出不是0均值的**，会导致后层的神经元的输入是非0均值的信号，这会对梯度产生影响。

  3. **计算复杂度高**，因为sigmoid函数是指数形式。

#### ReLU系

##### 神经科学解释

神经元的工作方式具有稀疏性和分布性——通过ReLU实现稀疏后的模型能更好地挖掘相关特征，拟合训练数据。

##### dead reLU problem

![[公式]](https://www.zhihu.com/equation?tex=W%3DW-lr%2A%5Cfrac%7B%5Cpartial+L%7D%7B%5Cpartial+W%7D)设激活函数为relu的层的权重为W，则W的权重更新公式为：

![[公式]](https://www.zhihu.com/equation?tex=W%3DW-lr%2A%5Cfrac%7B%5Cpartial+L%7D%7B%5Cpartial+W%7D)

假设学习率lr很大，梯度更新量也很大，此时W会发生非常大的更新，举个例子：

原始W=[1.5,2.5,1.4,5.5]

lr=10,

梯度更新量对应为[15,25,14,55]

则更新之后W全部为很大的负数了。

假设上一层的输出（也就是带有relu激活函数的层的输入）大都为正，这将会导致当前层的所有未激活函数输出均为负数，那么进入relu之后就被直接置0了，导致输出永远为0，梯度更新量永远为0，该层的权重系数不再发生变化。

- 解决方案：

1、leaky relu，elu甚至是random relu的使用是的输入是负数的时候激活函数也有输出；

2、避免使用太大的学习率

3、权重初始化策略不要初始化出太大的W（后续总结）

##### reLU

rectified linear unit，整流线性单元。是现代神经网络中最常用的激活函数，大多数前馈神经网络默认使用的激活函数。

单侧抑制：作为分段线性函数，把所有的负值都变为0，而正值不变。

![[公式]](https://www.zhihu.com/equation?tex=f%28x%29%3Dmax%280%2Cx%29)

- 导数

- 图像

  ![image-20201019123807214](/Users/huangjiani/Library/Application Support/typora-user-images/image-20201019123807214.png)

- 优点

  1. 使用ReLU的SGD算法的收敛速度比 sigmoid 和 tanh 快。

  2. 在x>0区域上，不会出现梯度饱和、梯度消失的问题。

  3. 计算复杂度低，不需要进行指数运算，只要一个阈值就可以得到激活值。

- 缺点

  1. ReLU的输出**不是0均值**的。

  2. ** Dead ReLU Problem(神经元坏死现象)**：ReLU在负数区域被kill的现象叫做dead relu。ReLU在训练的时很“脆弱”。在x<0时，梯度为0。这个神经元及之后的神经元梯度永远为0，不再对任何数据有所响应，导致相应参数永远不会被更新。

  **产生**这种现象的两个**原因**：参数初始化问题；learning rate太高导致在训练过程中参数更新太大。

  **解决方法**：采用Xavier初始化方法，以及避免将learning rate设置太大或使用adagrad等自动调节learning rate的算法。

##### leaky reLU

**渗漏整流线性单元**(Leaky ReLU)，为了解决dead ReLU现象。用一个类似0.01的小值来初始化神经元，从而**使得ReLU在负数区域更偏向于激活而不是死掉**。这里的斜率都是确定的。

f(x) = max(0.01x, x)

##### preLU

**参数整流线性单元**(Parametric Rectified linear unit，PReLU)，用来解决ReLU带来的神经元坏死的问题。

公式： ![[公式]](https://www.zhihu.com/equation?tex=f%28x%29%3Dmax%28%5Calpha+x%2Cx%29) 。 其中 ![[公式]](https://www.zhihu.com/equation?tex=%5Calpha) 不是固定的，是通过反向传播学习出来的。

##### eLU

**指数线性单元(ELU)：**具有relu的优势，没有Dead ReLU问题，输出均值接近0，实际上PReLU和Leaky ReLU都有这一优点。有负数饱和区域，从而对噪声有一些鲁棒性。可以看做是介于ReLU和Leaky ReLU之间的一个函数。当然，这个函数也需要计算exp，从而计算量上更大一些。

![img](https://pic1.zhimg.com/80/v2-7c560bb667e0eebe798dd383ecbe504c_1440w.jpg)

#### tanh

也称为双曲正切函数，取值范围为[-1,1]。

![img](https://pic2.zhimg.com/80/v2-4982b8b6783b823a5566b85a8eb22dbd_1440w.jpg)

- 导数

  ![img](https://pic1.zhimg.com/80/v2-a1850908aacbfdcabcc551ecc9196b14_1440w.jpg)

- 图像

  ![img](https://pic4.zhimg.com/80/v2-b3d1085558d8a5a00cf837cf93a8cacb_1440w.jpg)

- 优点

  1. 是sigmoid函数的变形

     ![img](https://pic4.zhimg.com/80/v2-ea2ac4e98733b5111b316a57b16c6eb3_1440w.jpg)

  2.  0 均值的，因此实际应用中 Tanh 会比 sigmoid 更好

- 缺点

  仍然存在**梯度饱和**与**exp计算**的问题

#### maxout

论文链接：[https://arxiv.org/pdf/1302.4389.pdf](https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1302.4389.pdf)

推荐一篇博客，讲的很好：[https://blog.csdn.net/hjimce/article/details/50414467](https://link.zhihu.com/?target=https%3A//blog.csdn.net/hjimce/article/details/50414467)

Maxout隐藏层每个神经元的计算公式如下：

![img](https://pic4.zhimg.com/80/v2-66d0957c21930babc764792b4fc0cfeb_1440w.jpg)

maxout的思路也不难，本身我们的激活函数就可以作为一个激活层来当做层使用，而maxout相对来说更灵活，直接将激活层的部分替换为了一个新的隐层结构如上图，简单粗暴点说就是用一个隐层来作为激活函数，而这个隐层的神经元个数就是maxout的K参数可以人工指定。

![img](https://pic4.zhimg.com/80/v2-f0fbfeca52e036da5624c60bee908ef3_1440w.jpg)

这一层的输出就是这样的，其中maxout层的权重w和b也是要进行权重初始化的，只不过在输出上使用了max来去激活值最大的输出从而使得 这一整个隐层变成了一个分段线性函数。

1、maxout激活函数并不是一个固定的函数，不像Sigmod、Relu、Tanh等函数，是一个固定的函数方程

2、它是一个可学习的激活函数，因为我们W参数是学习变化的。

3、它是一个分段线性函数：

![img](https://pic3.zhimg.com/80/v2-ceea98a79cb36b7dbc509f3774bcd26a_1440w.jpg)

然而任何一个凸函数，都可以由线性分段函数进行逼近近似。其实我们可以把以前所学到的激活函数：relu、abs激活函数，看成是分成两段的线性函数，如下示意图所示：

![img](https://pic1.zhimg.com/80/v2-a0acb89ba5a7f47376047c2b6007c978_1440w.jpg)



　maxout的拟合能力是非常强的，它可以拟合任意的的凸函数。最直观的解释就是任意的凸函数都可以由分段线性函数以任意精度拟合（学过高等数学应该能明白），而maxout又是取k个隐隐含层节点的最大值，这些”隐隐含层"节点也是线性的，所以在不同的取值范围下，最大值也可以看做是分段线性的（分段的个数与k值有关）

Maxout是通过分段线性函数来拟合所有可能的凸函数来作为激活函数的，但是由于线性函数是可学习，所以实际上是可以学出来的激活函数。具体操作是对所有线性取最大，也就是把若干直线的交点作为分段的边界，然后每一段取最大。

- 优点：

1. Maxout的拟合能力非常强，可以拟合任意的凸函数。

2. Maxout具有ReLU的所有优点，线性、不饱和性。

3. 不会出现神经元坏死的现象。

缺点：增加了参数量。常规的激活函数就只是单纯的函数而已,而maxout还需要反向传播去更新它自身的权重系数。

#### Softmax

归一化指数函数，是逻辑函数的一种推广。

用于多分类过程中，将多个神经元的输出，映射到(0,1)区间内，可以看成概率来理解。

假设我们有一个数组，V，Vi表示V中的第i个元素，那么这个元素的softmax值就是

![img](https://pic4.zhimg.com/80/v2-65035de6fdfd8b2f13b930191e9a548b_1440w.jpg)

- 求导

```
使用交叉熵误差作为softmax 函数的损失函数后，反向传播得到（y1 − t1, y2 − t2, y3 − t3）这样“ 漂亮”的结果。实际上，这样“漂亮”的结果并不是偶然的，而是为了得到这样的结果，特意设计了交叉熵误差函数。回归问题中输出层使用“恒等函数”，损失函数使用“平方和误差”，也是出于同样的理由（3.5 节）。也就是说，使用“平方和误差”作为“恒等函数”的损失函数，反向传播才能得到（y1 −t1, y2 − t2, y3 − t3）这样“漂亮”的结果。
```



### 损失函数

#### 交叉熵损失函数

#### MSE损失函数



## 反向传播算法推导















## 代码架构





## 实验优化说明

该部分为对作业要求“不同网络结果、网络参数的实验比较“的详细阐述。



## 对反向传播算法的理解



